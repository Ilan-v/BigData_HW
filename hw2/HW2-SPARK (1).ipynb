{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework 2: Map-reduce, Hadoop and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load the Microsoft.com data into HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "delete data folder and re-create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/stud15/ex2': File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir /user/stud15/ex2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put data and then show contents of folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 stud15 stud15        387 2022-05-20 12:42 /user/stud15/ex2/countries.txt\n",
      "-rw-r--r--   3 stud15 stud15    1491520 2022-05-20 12:42 /user/stud15/ex2/microsoft-com.data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "put: `/user/stud15/ex2/microsoft-com.data': File exists\n",
      "put: `/user/stud15/ex2/countries.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -put /home/stud15/bigdata_lab/HW2/microsoft-com.data /user/stud15/ex2/microsoft-com.data\n",
    "hdfs dfs -put /home/stud15/bigdata_lab/HW2/countries.txt /user/stud15/ex2/countries.txt\n",
    "hdfs dfs -ls /user/stud15/ex2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show last lines of data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V,42709,1003,1\n",
      "V,42710,1035,1\n",
      "V,42710,1001,1\n",
      "V,42710,1018,1\n",
      "V,42711,1008,1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -cat /user/stud15/ex2/microsoft-com.data | tail -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Parse the data according to the format specified in the .info file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'V', u'42711', u'1008', u'1'],\n",
       " [u'V', u'42710', u'1035', u'1'],\n",
       " [u'V', u'42710', u'1018', u'1'],\n",
       " [u'V', u'42710', u'1001', u'1'],\n",
       " [u'V', u'42709', u'1003', u'1']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_rdd = sc.textFile(\"hdfs:/user/stud15/ex2/microsoft-com.data\") # read data\n",
    "lst_rdd = raw_rdd.map(lambda line: line.replace('\"','').split(',')) # split each row by ,\n",
    "lst_rdd.top(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) only users that visited at least one page related to one of the given countries.\n",
    "Given the list of countries in the file countries.txt (e.g., South Africa, Spain, Sweden, Switzerland), filter the data to include only users that visited at least one page related to one of the given countries.\n",
    "\n",
    "**ASSUMPTION: the header is the same as the country name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_votes_by_countries(attr_rdd,votes_rdd,countries_rdd):\n",
    "    # return votes for attributes which contains country name\n",
    "    countries_lst = sc.broadcast(countries_rdd.collect()) # broadcast countries between nodes\n",
    "    relevant_lines = attr_rdd.filter(lambda x: x[3] in countries_lst.value) # filter attribute lines which contains countries\n",
    "    relevant_ids = relevant_lines.map(lambda x: x[1]) # get ids of relevant attribute line (which contains country)\n",
    "    country_ids = sc.broadcast(relevant_ids.collect()) # TODO: verify that we don't cheat!! (might be lots of records)\n",
    "    country_votes = votes_rdd.filter(lambda x: x[2] in country_ids.value) # return the votes\n",
    "    return country_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attr_rdd = lst_rdd.filter(lambda x: x[0]=='A') # filter attribute lines\n",
    "votes_rdd = lst_rdd.filter(lambda x: x[0]=='V') # filter votes lines\n",
    "countries_rdd = sc.textFile(\"hdfs:/user/stud15/ex2/countries.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'A', u'1297', u'1', u'Central America', u'/centroam'],\n",
       " [u'A', u'1295', u'1', u'Training', u'/train_cert'],\n",
       " [u'A', u'1294', u'1', u'Bookshelf', u'/bookshelf']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_rdd.top(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'V', u'42711', u'1008', u'1'],\n",
       " [u'V', u'42710', u'1035', u'1'],\n",
       " [u'V', u'42710', u'1018', u'1']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes_rdd.top(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Venezuela', u'Uruguay', u'UK']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_rdd.top(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_votes = filter_votes_by_countries(attr_rdd,votes_rdd,countries_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'V', u'42708', u'1123', u'1'],\n",
       " [u'V', u'42706', u'1059', u'1'],\n",
       " [u'V', u'42698', u'1005', u'1'],\n",
       " [u'V', u'42688', u'1053', u'1'],\n",
       " [u'V', u'42665', u'1023', u'1'],\n",
       " [u'V', u'42649', u'1223', u'1'],\n",
       " [u'V', u'42647', u'1023', u'1'],\n",
       " [u'V', u'42641', u'1053', u'1'],\n",
       " [u'V', u'42637', u'1053', u'1'],\n",
       " [u'V', u'42636', u'1053', u'1']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_votes.top(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_users = country_votes.map(lambda x: x[1]).distinct() # drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'42708',\n",
       " u'42706',\n",
       " u'42698',\n",
       " u'42688',\n",
       " u'42665',\n",
       " u'42649',\n",
       " u'42647',\n",
       " u'42641',\n",
       " u'42637',\n",
       " u'42636']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_users.top(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of relevant users: 3199, number of relevant votes: 3334\n"
     ]
    }
   ],
   "source": [
    "print(\"number of relevant users: %s, number of relevant votes: %s\"%(relevant_users.count(),country_votes.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter votes by the user ids we have found \n",
    "user_ids = sc.broadcast(relevant_users.collect()) \n",
    "relevant_data = votes_rdd.filter(lambda x: x[1] in user_ids.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'V', u'42708', u'1123', u'1'],\n",
       " [u'V', u'42708', u'1041', u'1'],\n",
       " [u'V', u'42708', u'1038', u'1'],\n",
       " [u'V', u'42708', u'1027', u'1'],\n",
       " [u'V', u'42708', u'1026', u'1']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_data.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13111"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a) For each user, the number of (unique) pages that he visited. \n",
    "For each user, the number of (unique) pages that he visited. show the top-10 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'40310', 35),\n",
       " (u'25185', 31),\n",
       " (u'12147', 30),\n",
       " (u'41066', 28),\n",
       " (u'12815', 28),\n",
       " (u'25922', 28),\n",
       " (u'10348', 28),\n",
       " (u'31809', 26),\n",
       " (u'19860', 26),\n",
       " (u'40122', 24)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use Map-Reduce in order to get the unique number of pages for each user:\n",
    "# The first map function's output is a tuple of (user, page_id), taking only unique page ids per each user\n",
    "# The second map function's output is a tuple of (user, 1)\n",
    "# The reduceByKey function sums the values of unique pages\n",
    "\n",
    "pages_per_user = votes_rdd.map(lambda x: (x[1],x[2])).distinct()\\\n",
    "                          .map(lambda x: (x[0], 1))\\\n",
    "                          .reduceByKey(lambda v1,v2: v1+v2)\\\n",
    "                          .sortBy(keyfunc=lambda x: -x[1]).take(10)\n",
    "\n",
    "pages_per_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.b) For each country, the number of users that visited a page of that country\n",
    "For each country, the number of users that visited a page of that country. Exclude from your\n",
    "report countries with a name longer than one word.\n",
    "\n",
    "**ASSUMPTION: country names are splitted only by spaces**\n",
    "\n",
    "**ASSUMPTION: titles are unique (validated on the last block)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_word_countries = countries_rdd.filter(lambda x: len(x.split(\" \")) < 2) # get only one-word countries\n",
    "one_word_lst = one_word_countries.collect() # small, constant size list - so it's fine to collect\n",
    "country_name_mapping = dict(attr_rdd.filter(lambda x: x[3] in one_word_lst) \\\n",
    ".map(lambda x: (x[1],x[3])).collect()) # dict of attribute line id -> country name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'1227': u'Argentina', u'1165': u'Poland', u'1166': u'Mexico', u'1084': u'UK', u'1223': u'Finland', u'1208': u'Israel', u'1080': u'Brazil', u'1262': u'Chile', u'1123': u'Germany', u'1267': u'Caribbean', u'1203': u'Denmark', u'1073': u'Taiwan', u'1209': u'Turkey', u'1217': u'Ireland', u'1053': u'Jakarta', u'1079': u'Australia', u'1059': u'Sweden', u'1195': u'Portugal', u'1194': u'China', u'1107': u'Slovakia', u'1112': u'Canada', u'1115': u'Hungary', u'1116': u'Switzerland', u'1258': u'Peru', u'1172': u'Belgium', u'1179': u'Colombia', u'1152': u'Russia', u'1229': u'Uruguay', u'1153': u'Venezuela', u'1188': u'Korea', u'1023': u'Spain', u'1183': u'Italy', u'1180': u'Slovenija', u'1241': u'India', u'1240': u'Thailand', u'1105': u'France', u'1005': u'Norway'}\n"
     ]
    }
   ],
   "source": [
    "print(country_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_votes_one = filter_votes_by_countries(attr_rdd,votes_rdd,one_word_countries) # relevant vote lines for one word countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'42708', u'1123'],\n",
       " [u'42706', u'1059'],\n",
       " [u'42698', u'1005'],\n",
       " [u'42688', u'1053'],\n",
       " [u'42665', u'1023']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid_vid = country_votes_one.map(lambda x: x[1:3]) # user id and attribute line id\n",
    "uid_vid.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'42708', u'Germany'),\n",
       " (u'42706', u'Sweden'),\n",
       " (u'42698', u'Norway'),\n",
       " (u'42688', u'Jakarta'),\n",
       " (u'42665', u'Spain')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid_country = uid_vid.map(lambda x: (x[0], country_name_mapping[x[1]])) # use the dict we created to get country name\n",
    "uid_country.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uid_country_unique = uid_country.distinct() # count only once each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Brazil', 121),\n",
       " (u'Canada', 128),\n",
       " (u'Italy', 167),\n",
       " (u'Peru', 3),\n",
       " (u'France', 183),\n",
       " (u'Slovakia', 11),\n",
       " (u'Ireland', 13),\n",
       " (u'Caribbean', 5),\n",
       " (u'Argentina', 32),\n",
       " (u'Venezuela', 8),\n",
       " (u'Israel', 34),\n",
       " (u'Korea', 94),\n",
       " (u'Norway', 42),\n",
       " (u'Germany', 372),\n",
       " (u'Chile', 4),\n",
       " (u'Denmark', 55),\n",
       " (u'Belgium', 45),\n",
       " (u'Thailand', 11),\n",
       " (u'Poland', 38),\n",
       " (u'Spain', 191),\n",
       " (u'UK', 186),\n",
       " (u'Jakarta', 670),\n",
       " (u'Turkey', 9),\n",
       " (u'Finland', 29),\n",
       " (u'Sweden', 258),\n",
       " (u'Australia', 136),\n",
       " (u'Switzerland', 31),\n",
       " (u'Russia', 52),\n",
       " (u'Portugal', 15),\n",
       " (u'Mexico', 33),\n",
       " (u'Uruguay', 4),\n",
       " (u'India', 9),\n",
       " (u'China', 26),\n",
       " (u'Colombia', 11),\n",
       " (u'Hungary', 15),\n",
       " (u'Taiwan', 204),\n",
       " (u'Slovenija', 9)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = uid_country_unique.map(lambda x: x[1])\n",
    "sum_by_country = countries.countByValue()\n",
    "sum_by_country.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.c) The top 5 visited countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_votes = filter_votes_by_countries(attr_rdd,votes_rdd,countries_rdd)\n",
    "country_votes_ids = country_votes.map(lambda x: x[2])\n",
    "votes_count = country_votes_ids.countByValue() # returns a python dict - it's fine, because we deal with small dict\n",
    "top_5_votes_count = sorted(votes_count.items(), key=lambda x:x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 visited countries:\n",
      "1. Jakarta: 670\n",
      "2. Germany: 372\n",
      "3. Sweden: 258\n",
      "4. Taiwan: 204\n",
      "5. Spain: 191\n"
     ]
    }
   ],
   "source": [
    "print(\"The top 5 visited countries:\")\n",
    "for i in range(5):\n",
    "    country_id, cnt = top_5_votes_count[i]\n",
    "    print(\"%s. %s: %s\"%(i+1,country_name_mapping[country_id],cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Write the report (4.a) to HDFS for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: based on 4.a which we skipped...\n",
    "pages_per_user.saveAsTextFile(\"hdfs:/user/stud15/ex2/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 stud15 stud15          0 2022-05-20 12:43 /user/stud15/ex2/output/_SUCCESS\n",
      "-rw-r--r--   3 stud15 stud15      33160 2022-05-20 12:43 /user/stud15/ex2/output/part-00000\n",
      "-rw-r--r--   3 stud15 stud15      31920 2022-05-20 12:43 /user/stud15/ex2/output/part-00001\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user/stud15/ex2/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *verify that titles are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of country titels: 41\n",
      "number of distinct country titels: 41\n",
      "they are equal - so the assumption that the titles are unique holds...\n"
     ]
    }
   ],
   "source": [
    "countries_rdd = sc.textFile(\"hdfs:/user/stud15/ex2/countries.txt\")\n",
    "countries_lst = sc.broadcast(countries_rdd.collect())\n",
    "tmp  = attr_rdd.filter(lambda x: x[3] in countries_lst.value).map(lambda x: x[3])\n",
    "print(\"number of country titels: %s\"%(tmp.count()))\n",
    "print(\"number of distinct country titels: %s\"%(tmp.distinct().count()))\n",
    "assert(tmp.count() == tmp.distinct().count())\n",
    "print(\"they are equal - so the assumption that the titles are unique holds...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) The average number of visits per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of visits per country is: 81.3170731707\n"
     ]
    }
   ],
   "source": [
    "# creating a pair rdd with (page_id, user_id)\n",
    "pages_users = votes_rdd.map(lambda x: (x[2], x[1]))\n",
    "# creating a pair rdd with (page_id, country)\n",
    "pages_countries = attr_rdd.filter(lambda x: x[3] in countries_lst.value).map(lambda x: (x[1], x[3]))\n",
    "country_visits = pages_users.join(pages_countries) # join of the 2 rdds by page_id\n",
    "\n",
    "# The map function's output is a tuple of (country, 1)\n",
    "# The reduce function sums the number of visits for each country\n",
    "visits_per_country = country_visits.map(lambda x: (x[1][1], 1)).reduceByKey(lambda v1,v2: v1+v2)\n",
    "\n",
    "# Calculating the average number of pages per user\n",
    "avg_visits_per_country = float(visits_per_country.map(lambda x: x[1]).mean())\n",
    "print \"The average number of visits per country is: {}\".format(avg_visits_per_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) The page id with minimum number of visits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if there are pages with 0 visits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9 pages with 0 visits. \n",
      "For example: page id 1291\n"
     ]
    }
   ],
   "source": [
    "visited_pages = votes_rdd.map(lambda x: x[2]).distinct().sortBy(keyfunc=lambda x: x)\n",
    "all_pages = attr_rdd.map(lambda x: x[1]).sortBy(keyfunc=lambda x: x)\n",
    "pages_with_0_visits = all_pages.subtract(visited_pages)\n",
    "\n",
    "if pages_with_0_visits.count() > 0:\n",
    "    zero_visits = True\n",
    "    print \"There are {} pages with 0 visits. \\n\" \\\n",
    "            \"For example: page id {}\".format(pages_with_0_visits.count(), pages_with_0_visits.takeSample(False, 1, 42)[0])\n",
    "else:\n",
    "    zero_visits = False\n",
    "    print \"There are no pages with 0 visits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case that there are no pages with 0 visits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The map function's output is a tuple of (page_id, 1)\n",
    "# The reduce function sums the number of visits for each page\n",
    "# The sort function gives us the page_id with the lowest number of visits\n",
    "\n",
    "if not zero_visits:\n",
    "    visits_per_page = votes_rdd.map(lambda tpl: (tpl[2], 1))\\\n",
    "                                   .reduceByKey(lambda v1,v2: v1+v2)\\\n",
    "                                   .sortBy(keyfunc=lambda x: -x[1], ascending=False).take(1)\n",
    "\n",
    "    visits_per_page#.collect()\n",
    "    print \"The page id with the lowest number of visits is page {}\\n\" \\\n",
    "          \"Which had {} visits in total\".format(visits_per_page[0][0], visits_per_page[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) The average number of pages per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of pages per user is: 3.01592736388\n"
     ]
    }
   ],
   "source": [
    "# The first map function's output is a tuple of (user, page_id), taking only unique page ids per each user\n",
    "# The second map function's output is a tuple of (user, 1)\n",
    "# The reduceByKey function sums the values of unique pages\n",
    "\n",
    "pages_per_user = votes_rdd.map(lambda x: (x[1],x[2])).distinct()\\\n",
    "                          .map(lambda x: (x[0], 1))\\\n",
    "                          .reduceByKey(lambda v1,v2: v1+v2)\n",
    "\n",
    "# Calculating the average number of pages per user\n",
    "avg_pages_per_user = float(pages_per_user.map(lambda tpl: tpl[1]).mean())\n",
    "\n",
    "print \"The average number of pages per user is: {}\".format(avg_pages_per_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing our directories from HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/20 12:43:42 INFO fs.TrashPolicyDefault: Moved: 'hdfs://bdl0.eng.tau.ac.il:8020/user/stud15/ex2' to trash at: hdfs://bdl0.eng.tau.ac.il:8020/user/stud15/.Trash/Current/user/stud15/ex2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r /user/stud15/ex2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark2(2.1.0)",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
